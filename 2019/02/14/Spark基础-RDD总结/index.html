<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta name="baidu-site-verification" content="aU3q538RMY">
<meta http-equiv="X-UA-Compatible" content="ie=edge">



<title>Spark基础-RDD总结 | EgoSay</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


</head>
<body>
    <!--<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/canvas-nest.js/2.0.4/canvas-nest.js"></script>-->
	<!--<script src="https://cdn.bootcss.com/canvas-nest.js/2.0.4/canvas-nest.js"></script>-->
	<script type="text/javascript" src="/js/canvas-colorful-nest.js"></script>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">EgoSay&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/categories">Categories</a>
                
                    <a class="menu-item" href="/tags">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/books">Reading</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">EgoSay&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/categories">Categories</a>
                
                    <a class="menu-item" href="/tags">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                    <a class="menu-item" href="/books">Reading</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Spark基础-RDD总结</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">EgoSay</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">February 14, 2019&nbsp;&nbsp;21:22:22</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Learning/">Learning</a>
                            
                        </span>
						<span class="post-count">
					Words:
						<font color="#2d96bd">2.2k</font>
					</span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="RDD总结"><a href="#RDD总结" class="headerlink" title="RDD总结"></a>RDD总结</h1><h2 id="抽象类RDD源码分析"><a href="#抽象类RDD源码分析" class="headerlink" title="抽象类RDD源码分析"></a>抽象类RDD源码分析</h2><h3 id="基本定义"><a href="#基本定义" class="headerlink" title="基本定义"></a>基本定义</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private var _sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</span></span></span><br><span class="line"><span class="class"><span class="params">  </span>) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span></span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h3><ul>
<li><p><strong><code>id</code>: 当前RDD的唯一身份标识</strong>, <code>val id: Int = sc.newRddId()</code></p>
</li>
<li><p><code>storageLevel</code>: 当前RDD的存储级别, <code>private var storageLevel: StorageLevel = StorageLevel.NONE</code></p>
</li>
<li><p><code>creationSite</code>: 创建当前RDD的用户代码(e.g. <code>textFile</code>, <code>parallelize</code>)</p>
</li>
<li><p><code>scope</code>: 当前RDD的操作作用域, 有一个withScope调用, 就像是一个 AOP，嵌入到所有RDD 的转换和操作的函数中，RDDOperationScope会把调用栈记录下来，用于绘制Spark UI的 DAG</p>
</li>
<li><p><code>checkpointData</code>: 当前RDD的检查点数据</p>
</li>
<li><p><code>checkpointAllMarkedAncestors</code>: 是否对所有标记了需要保存检查点的祖先保存检查点</p>
</li>
<li><p><code>doCheckpointCalled</code>: 是否已经调用了doCheckPoint方法设置检查点, 此属性可以阻止对RDD多次设置检查点</p>
</li>
<li><p><strong><code>stateLock</code>: 用于锁定RDD的可变状态</strong>, 它被定义为一个常量整数,<code>private val stateLock = new Integer(0)</code> , 联想到操作系统中的信号量机制,在进行可变操作时会先执行<code>stateLock.synchronized{}</code>加锁, 那么为什么直接用<code>this</code>给对象加锁呢？源码里是这样解释的:</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.).  We do</span></span><br><span class="line"><span class="comment">   * not use `this` because RDDs are user-visible, so users might have added their own locking on</span></span><br><span class="line"><span class="comment">   * RDDs; sharing that could lead to a deadlock.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * One thread might hold the lock on many of these, for a chain of RDD dependencies; but</span></span><br><span class="line"><span class="comment">   * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no</span></span><br><span class="line"><span class="comment">   * chance of deadlock.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * The use of Integer is simply so this is serializable -- executors may reference the shared</span></span><br><span class="line"><span class="comment">   * fields (though they should never mutate them, that only happens on the driver).</span></span><br><span class="line"><span class="comment">   */</span></span><br></pre></td></tr></table></figure>

<p>  大致意思就是因为RDD都是用户可见的, 用户线程和spark的调度器线程可以同时访问和修改RDD依赖项和分区, 所以用户可能在某些时候会主动加锁，而RDD是一个并行数据结构，大家都去抢夺锁的话就有可能会产生死锁(理解的比较片面，后续需要回顾理解, 详情见<a href="https://github.com/apache/spark/commit/0da667d31436c43e06cb6bb5ac65a17f65edd08b" target="_blank" rel="noopener">SPARK-28917</a>)</p>
</li>
<li><p><code>barrier_</code>: 一个还在实验的特性</p>
</li>
</ul>
<hr>
<h3 id="一些函数方法定义"><a href="#一些函数方法定义" class="headerlink" title="一些函数方法定义"></a>一些函数方法定义</h3><p>RDD采用了模版方法的模式设计，抽象类RDD定义了模版方法及一些接口</p>
<h4 id="一些接口"><a href="#一些接口" class="headerlink" title="一些接口"></a>一些接口</h4><ul>
<li><code>compute</code>: 对RDD的分区进行计算</li>
<li><code>getPartitions</code>: 获取当前RDD的所有分区</li>
<li><code>getDependencies</code>: 获取当前RDD的所有依赖</li>
<li><code>getPreferredLocations</code>: 获取某一分区的偏好位置</li>
</ul>
<h4 id="一些模版方法"><a href="#一些模版方法" class="headerlink" title="一些模版方法"></a>一些模版方法</h4><ul>
<li><p><code>partitions</code>方法<br>用于获取RDD的分区数组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">partitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line">    checkpointRDD.map(_.partitions).getOrElse &#123;</span><br><span class="line">      <span class="keyword">if</span> (partitions_ == <span class="literal">null</span>) &#123;</span><br><span class="line">        stateLock.synchronized &#123;</span><br><span class="line">          <span class="keyword">if</span> (partitions_ == <span class="literal">null</span>) &#123;</span><br><span class="line">            partitions_ = getPartitions</span><br><span class="line">            partitions_.zipWithIndex.foreach &#123;...&#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      partitions_</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出, <code>partitions</code>方法查找分区数组的优先级为：<br>从<code>CheckPoint</code>查找 -&gt; 读取<code>partitions_</code>属性 -&gt; 调用<code>getPartitions</code>方法获取</p>
</li>
<li><p><code>dependencies</code>方法<br>用于获取当前RDD的所有依赖的序列</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">dependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = &#123;</span><br><span class="line">    checkpointRDD.map(r =&gt; <span class="type">List</span>(<span class="keyword">new</span> <span class="type">OneToOneDependency</span>(r))).getOrElse &#123;</span><br><span class="line">      <span class="keyword">if</span> (dependencies_ == <span class="literal">null</span>) &#123;</span><br><span class="line">        stateLock.synchronized &#123;</span><br><span class="line">          <span class="keyword">if</span> (dependencies_ == <span class="literal">null</span>) &#123;</span><br><span class="line">            dependencies_ = getDependencies</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      dependencies_</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h4><ul>
<li><code>getStorageLevel</code><br>返回当前RDD的StorageLevel</li>
<li><code>getNumPartitions</code><br>获取这个RDD的分区数</li>
<li><code>getNarrowAncestors</code><br>利用 <code>DFS</code>算法遍历当前RDD的依赖树获取其祖先依赖中属于窄依赖的RDD序列<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getNarrowAncestors</span></span>: <span class="type">Seq</span>[<span class="type">RDD</span>[_]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ancestors = <span class="keyword">new</span> mutable.<span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[<span class="type">NarrowDependency</span>[_]])</span><br><span class="line">      <span class="keyword">val</span> narrowParents = narrowDependencies.map(_.rdd)</span><br><span class="line">      <span class="keyword">val</span> narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains)</span><br><span class="line">      narrowParentsNotVisited.foreach &#123; parent =&gt;</span><br><span class="line">        ancestors.add(parent)</span><br><span class="line">        visit(parent)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    visit(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In case there is a cycle, do not include the root itself</span></span><br><span class="line">    ancestors.filterNot(_ == <span class="keyword">this</span>).toSeq</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="还有一些基本算子"><a href="#还有一些基本算子" class="headerlink" title="还有一些基本算子"></a>还有一些基本算子</h4><hr>
<h2 id="为什么需要RDD"><a href="#为什么需要RDD" class="headerlink" title="为什么需要RDD"></a>为什么需要RDD</h2><p>从以下四个点解释</p>
<h3 id="数据处理模型"><a href="#数据处理模型" class="headerlink" title="数据处理模型"></a>数据处理模型</h3><p>通常数据处理的模型包括迭代计算、关系查询、MapReduce、流式处理等。Hadoop采用MapReduce模型，Storm采用流式处理模型，而Spark借助RDD实现了以上所有模型</p>
<h3 id="依赖划分原则"><a href="#依赖划分原则" class="headerlink" title="依赖划分原则"></a>依赖划分原则</h3><p>一个RDD包含一个或多个分区, 每个分区实际上是一个数据集合的片段, RDD之间通过依赖关系串联起来构建成DAG.</p><p><br>依赖划分为<code>NarrowDependcy和ShuffleDependcy</code>两种, 参考：</p><p><br><a href="#RDD的依赖">RDD的依赖</a></p><p><br><a href="https://blog.csdn.net/itfootball/article/details/73844521" target="_blank" rel="noopener">Spark宽依赖与窄依赖</a></p>
<h3 id="数据处理效率"><a href="#数据处理效率" class="headerlink" title="数据处理效率"></a>数据处理效率</h3><p><code>ShuffleDependcy</code>所依赖的上游RDD的计算过程允许在多个节点并发执行, 数据量大的时候可以通过适当调整分区数量，能有效提高Spark的数据处理效率</p>
<h3 id="容错处理"><a href="#容错处理" class="headerlink" title="容错处理"></a>容错处理</h3><p>传统关系型数据库往往采用日志记录的方式来容灾，Hadoop通过将数据备份到其他机器容灾。Spark则通过RDD依赖组成的DAG直接重新调度计算失败的Task，成功的Task可以从<code>CheckPoint</code>中读取。<br>(:在流式计算中，Spark需要通过记录日志和CheckPoint进行数据恢复</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.whizlabs.com/blog/spark-rdd/" target="_blank" rel="noopener">What is Spark RDD and Why Do We Need it?</a></p>
<hr>
<h2 id="RDD的依赖"><a href="#RDD的依赖" class="headerlink" title="RDD的依赖"></a>RDD的依赖</h2><h3 id="Dependency抽象类"><a href="#Dependency抽象类" class="headerlink" title="Dependency抽象类"></a><code>Dependency</code>抽象类</h3><p>Spark使用<code>Dependency</code>来表示RDD之间的依赖关系，其定义如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeveloperApi</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Dependency</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>抽象类Dependendcy只定义了一个名叫rdd的方法, 此方法返回当前依赖的RDD<br>依赖又分为<code>窄依赖(NarrowDependcy)</code>和<code>宽依赖(ShuffleDependcy)</code></p>
<h3 id="窄依赖-NarrowDependcy"><a href="#窄依赖-NarrowDependcy" class="headerlink" title="窄依赖(NarrowDependcy)"></a><code>窄依赖(NarrowDependcy)</code></h3><h4 id="基本定义-1"><a href="#基本定义-1" class="headerlink" title="基本定义"></a>基本定义</h4><p>如果<code>子RDD</code>依赖于<code>父RDD</code>中固定的<code>Partion</code>分区, 其之间的依赖关系属于窄依赖, 定义如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">_rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">T</span>] </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Get the parent partitions for a child partition.</span></span><br><span class="line"><span class="comment">   * @param partitionId a partition of the child RDD</span></span><br><span class="line"><span class="comment">   * @return the partitions of the parent RDD that the child partition depends upon</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">Seq</span>[<span class="type">Int</span>]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">T</span>] = _rdd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>还有两个子类<code>OneToOneDependency</code> 和 <code>RangeDependency</code></p>
<h4 id="OneToOneDependency"><a href="#OneToOneDependency" class="headerlink" title="OneToOneDependency"></a><code>OneToOneDependency</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a one-to-one dependency between partitions of the parent and child RDDs.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(partitionId)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个类重写了<code>getParents</code>, 直接把传递进去的id号封装成Seq返回。可以是多个子RDD, 但每一个子RDD的每一个分区和父RDD都是一对一的关系, 子RDD分区的个数和父RDD分区的个数一样, 如下两种情况<br><img src="https://cjwdream.top/image/SparkNarrowDependcy1.jpg" alt></p>
<p><img src="https://cjwdream.top/image/SparkNarrowDependcy2.jpg" alt></p>
<h4 id="RangeDependency"><a href="#RangeDependency" class="headerlink" title="RangeDependency"></a><code>RangeDependency</code></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a one-to-one dependency between ranges of partitions in the parent and child RDDs.</span></span><br><span class="line"><span class="comment"> * @param rdd the parent RDD</span></span><br><span class="line"><span class="comment"> * @param inStart the start of the range in the parent RDD</span></span><br><span class="line"><span class="comment"> * @param outStart the start of the range in the child RDD</span></span><br><span class="line"><span class="comment"> * @param length the length of the range</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RangeDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>], inStart: <span class="type">Int</span>, outStart: <span class="type">Int</span>, length: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123;</span><br><span class="line">      <span class="type">List</span>(partitionId - outStart + inStart)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Nil</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>RangeDependency</code>也是重写了<code>getParents</code>方法, 与<code>OneToOneDependency</code>不同的点在于可能有多个父RDD, 而一个子RDD的分区依赖于很多父RDD的分区, 但分区也都是一对一的关系, 即子RDD的每一个分区都且唯一对应着一个父RDD的分区, union操作中存在这种情况<br><img src="https://cjwdream.top/image/SparkNarrowDependcy3.jpg" alt></p>
<h3 id="宽依赖-ShuffleDependcy"><a href="#宽依赖-ShuffleDependcy" class="headerlink" title="宽依赖(ShuffleDependcy)"></a><code>宽依赖(ShuffleDependcy)</code></h3><p>父RDD的一个分区中的数据同时被子RDD的多个分区所依赖, 就是说子RDD的分区和父RDD不再是一对一的关系, 可能是多对一的关系, 数据被切分利用</p>
<h4 id="源码实现"><a href="#源码实现" class="headerlink" title="源码实现"></a>源码实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a dependency on the output of a shuffle stage. Note that in the case of shuffle,</span></span><br><span class="line"><span class="comment"> * the RDD is transient since we don't need it on the executor side.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param _rdd the parent RDD</span></span><br><span class="line"><span class="comment"> * @param partitioner partitioner used to partition the shuffle output</span></span><br><span class="line"><span class="comment"> * @param serializer [[org.apache.spark.serializer.Serializer Serializer]] to use. If not set</span></span><br><span class="line"><span class="comment"> *                   explicitly then the default serializer, as specified by `spark.serializer`</span></span><br><span class="line"><span class="comment"> *                   config option, will be used.</span></span><br><span class="line"><span class="comment"> * @param keyOrdering key ordering for RDD's shuffles</span></span><br><span class="line"><span class="comment"> * @param aggregator map/reduce-side aggregator for RDD's shuffle</span></span><br><span class="line"><span class="comment"> * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="class"><span class="params">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="class"><span class="params">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">    require(aggregator.isDefined, <span class="string">"Map-side combine without Aggregator specified!"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">rdd</span></span>: <span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] = _rdd.asInstanceOf[<span class="type">RDD</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> keyClassName: <span class="type">String</span> = reflect.classTag[<span class="type">K</span>].runtimeClass.getName</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> valueClassName: <span class="type">String</span> = reflect.classTag[<span class="type">V</span>].runtimeClass.getName</span><br><span class="line">  <span class="comment">// Note: It's possible that the combiner class tag is null, if the combineByKey</span></span><br><span class="line">  <span class="comment">// methods in PairRDDFunctions are used instead of combineByKeyWithClassTag.</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">val</span> combinerClassName: <span class="type">Option</span>[<span class="type">String</span>] =</span><br><span class="line">    <span class="type">Option</span>(reflect.classTag[<span class="type">C</span>]).map(_.runtimeClass.getName)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> shuffleId: <span class="type">Int</span> = _rdd.context.newShuffleId()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> shuffleHandle: <span class="type">ShuffleHandle</span> = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, <span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(<span class="keyword">this</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="属性解释"><a href="#属性解释" class="headerlink" title="属性解释"></a>属性解释</h4><ol>
<li><code>_rdd</code>：泛型要求必须是Product2[K, V]及其子类的RDD</li>
<li><code>partitioner</code>：分区计算器Partitioner</li>
<li><code>serializer</code>：SparkEnv中创建的serializer，即org.apache.spark.serializer.JavaSerializer。</li>
<li><code>keyOrdering</code>：按照K进行排序的scala.math.Ordering的实现类。</li>
<li><code>aggregator</code>：对map任务的输出数据进行聚合的聚合器。</li>
<li><code>mapSideCombine</code>：是否在map端进行合并，默认为false。</li>
<li><code>keyClassName</code>：K的类名。</li>
<li><code>valueClassName</code>：V的类名。</li>
<li><code>combinerClassName</code>：组合器，将切割后的数据组合在一个RDD所使用的组合器</li>
<li><code>shuffleId</code>：当前ShuffleDependency的身份标识。</li>
<li><code>shuffleHandle</code>：当前ShuffleDependency的处理器</li>
</ol>
<h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><ol>
<li>ShuffleDependency需要经过shuffle过程才能形成，而shuffle都是基于 PairRDD进行的，所以传入的RDD需要是key-value类型的</li>
<li><code>newShuffleId</code>的作用是得到唯一的shufflId（每次加1）</li>
</ol>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>EgoSay</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://cjwdream.top/2019/02/14/Spark基础-RDD总结/">http://cjwdream.top/2019/02/14/Spark基础-RDD总结/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Spark/"># Spark</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2019/02/19/Spark-Caching-and-Checkpoint-Mechanism/">Spark基础-Spark缓存和checkpoint机制</a>
            
            
            <a class="next" rel="next" href="/2019/01/19/Spark基础-Spark中的一些基本概念/">Spark基础-Spark基本概念</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
   <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
   <!--<span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>-->
    <div class="copyright">
        <span>© EgoSay | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
   		<span class="post-count"> | 本站字数统计:<font color="#2d96bd">126.4k</font></span>
		<!--<span id="busuanzi_container_site_uv"> | 本站访客数:<font color="#2d96bd"><span id="busuanzi_value_site_uv"></span></font>人次-->

    </div>
</footer>

    </div>
</body>
</html>
